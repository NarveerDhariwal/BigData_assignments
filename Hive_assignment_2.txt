


Scenario Based questions:

Q: Will the reducer work or not if you use “Limit 1” in any HiveQL query?

A: It depends on our query execution plan. If we have only select then reducer will not work but if are doing data shuffling,operation like group by, aggregation functions then reducer will work.



Q: Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 

A: When multiple clients try to access Hive at the same time, they interact with the shared metastore to retrieve metadata information and execute queries. The metastore ensures data consistency and handles concurrent access by using locking mechanisms. The metastore database (such as Apache Derby, MySQL, or PostgreSQL) manages the concurrency control internally, allowing multiple clients to access and modify the metadata concurrently.




Q: Suppose, I create a table that contains details of all the transactions done by the customers: 
CREATE TABLE transaction_details 
(cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. 
But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?

A: --> Enable dynamic partion and partition the table on month column
   --> use compressed file format like orc, parquet, avro
   --> optimise hive parameters such as the number of reducers, memory allocation.




Q: How can you add a new partition for the month December in the above partitioned table?

I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?

A: By adding a static partition column, setting its value during data insertion, and enabling dynamic partitioning with nonstrict mode, we can successfully insert data into partitions dynamically without encountering the "Dynamic partition strict mode requires at least one static partition column" error.





Q: Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
   id first_name last_name email gender ip_address
   How will you consume this CSV file into the Hive warehouse using built-in SerDe?

A: we can create hive external table and then perform load from the file location

CREATE EXTERNAL TABLE sample_table (
  id INT,
  first_name STRING,
  last_name STRING,
  email STRING,
  gender STRING,
  ip_address STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/temp';

LOAD DATA INPATH '/temp/sample.csv' INTO TABLE sample_table;





Q: Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
   So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

A: By employing strategies, such as merging small files, using CombineFileInputFormat, leveraging columnar file formats, and utilizing Hive's partitioning, we can create a single Hive table for lots of small files without degrading system performance.





Q: LOAD DATA LOCAL INPATH ‘Home/country/state/’
   OVERWRITE INTO TABLE address;
 
   The following statement failed to execute. What can be the cause?

A: Missing lead slash('/') indicating root directory. ‘Home/country/state/’ should be '/Home/country/state/’ or 'file:///Home/country/state/’






Q: Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?

A: Yes it is possible. We need to choose 100 new machines that meet the hardware and network requirements for the Hadoop cluster and are accessible and can be integrated into the existing network infrastructure.
Install Hadoop on the new machines, following the same version and configuration as the existing cluster. This includes setting up HDFS, YARN, and other necessary components
Update the configuration files on the new machines to point to the existing Hadoop cluster. This includes updating the core-site.xml, hdfs-site.xml, and other relevant configuration files to match the settings of the existing cluster. Ensure that the new nodes can communicate with the existing nodes and access the necessary resources.











Hive Practical questions:

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

Answer: 


hive> show create table movies_data;
OK
CREATE TABLE `movies_data`(
  `movie_id` int, 
  `title` string, 
  `genres` array<string>)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
WITH SERDEPROPERTIES ( 
  'collection.delim'='|', 
  'field.delim'=',', 
  'serialization.format'=',') 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost/user/hive/warehouse/hive_db.db/movies_data'
TBLPROPERTIES (
  'bucketing_version'='2', 
  'last_modified_by'='abc', 
  'last_modified_time'='1684121772', 
  'skip.header.line.count'='1', 
  'transient_lastDdlTime'='1684121790')
Time taken: 0.14 seconds, Fetched: 22 row(s)
hive> 

hive> show create table movie_ratings;
OK
CREATE TABLE `movie_ratings`(
  `userid` int, 
  `movieid` int, 
  `rating` float, 
  `time_stamp` bigint)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
WITH SERDEPROPERTIES ( 
  'field.delim'=',', 
  'serialization.format'=',') 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost/user/hive/warehouse/hive_db.db/movie_ratings'
TBLPROPERTIES (
  'bucketing_version'='2', 
  'skip.header.line.count'='1', 
  'transient_lastDdlTime'='1684122242')
Time taken: 0.066 seconds, Fetched: 20 row(s)


Inner join: hive> select distinct(t1.movie_id, t2.rating) from movies_data t1 join movie_ratings t2 on t1.movie_id = t2.movieId;

{"col1":193585,"col2":3.5}
{"col1":193587,"col2":3.5}
{"col1":193609,"col2":4.0}
Time taken: 36.252 seconds, Fetched: 30417 row(s)

Left join: select distinct(t1.movie_id, t2.rating) from movie_data t1 left join movie_ratings t2 on t1.movie_id = t2.movieId;

{"col1":193583,"col2":3.5}
{"col1":193585,"col2":3.5}
{"col1":193587,"col2":3.5}
{"col1":193609,"col2":4.0}
Time taken: 36.242 seconds, Fetched: 30435 row(s)

Right join: select distinct(t1.movie_id, t2.rating) from movie_data t1 right join movie_ratings t2 on t1.movie_id = t2.movieId;

{"col1":193585,"col2":3.5}
{"col1":193587,"col2":3.5}
{"col1":193609,"col2":4.0}
Time taken: 36.453 seconds, Fetched: 30417 row(s)

Full join: select distinct(t1.movie_id, t2.rating) from movie_data t1 full join movie_ratings t2 on t1.movie_id = t2.movieId;

{"col1":193583,"col2":3.5}
{"col1":193585,"col2":3.5}
{"col1":193587,"col2":3.5}
{"col1":193609,"col2":4.0}
Time taken: 50.818 seconds, Fetched: 30435 row(s)






BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset 

CREATE TABLE air_quality (
  Date1 STRING,
  Time1 STRING,
  CO_GT FLOAT,
  PT08_S1_CO INT,
  NMHC_GT INT,
  C6H6_GT FLOAT,
  PT08_S2_NMHC INT,
  NOx_GT INT,
  PT08_S3_NOx INT,
  NO2_GT INT,
  PT08_S4_NO2 INT,
  PT08_S5_O3 INT,
  T FLOAT,
  RH FLOAT,
  AH FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
tblproperties ("skip.header.line.count" = "1");


2. try to place a data into table location

load data local inpath 'file:///config/workspace/AirQualityUCI_new.csv' into table air_quality;





3. Perform a select operation . 

hive> select * from air_quality limit 2;
OK
10-03-04        18:00:00        2.6     1360    150     11.9    1046    166     1056    113     1692    1268    13.6    48.9    0.7578
10-03-04        19:00:00        2.0     1292    112     9.4     955     103     1174    92      1559    972     13.3    47.7    0.7255
Time taken: 0.145 seconds, Fetched: 2 row(s)







4. Fetch the result of the select operation in your local as a csv file .

hive> INSERT OVERWRITE  DIRECTORY 'file:///config/workspace/output.csv' row format delimited fields terminated by ','  SELECT * FROM air_quality WHERE c6h6_gt >10;


To convert above text file into csv format:
sed 's/\t/,/g' '/config/workspace/output.csv/000000_0' > '/config/workspace/file.csv'





5. Perform group by operation . 

hive> select date1, round(sum(CO_GT),2), sum(PT08_S1_CO) from air_quality
    > group by date1;





7. Perform filter operation at least 5 kinds of filter examples .

hive> SELECT * FROM air_quality WHERE CO_GT > 2.5 limit 3;
OK
10-03-04        18:00:00        2.6     1360    150     11.9    1046    166     1056    113     1692    1268    13.6    48.9    0.7578
11-03-04        14:00:00        2.9     1371    164     11.5    1034    207     983     128     1730    1037    8.0     81.1    0.8736
11-03-04        17:00:00        2.9     1383    150     11.2    1020    243     1008    135     1719    1104    9.8     67.6    0.8185
Time taken: 0.213 seconds, Fetched: 3 row(s)
hive> SELECT * FROM air_quality WHERE NMHC_GT < 100 AND PT08_S2_NMHC > 1200 limit 3;
OK
12-03-04        9:00:00 -200.0  1545    -200    22.1    1353    -200    767     -200    2058    1588    9.2     56.2    0.6561
18-03-04        10:00:00        4.5     1617    -200    21.3    1333    349     686     150     2010    1819    17.8    40.5    0.821
18-03-04        16:00:00        2.8     1496    -200    16.8    1205    172     822     169     1767    1347    27.1    23.1    0.818
Time taken: 0.209 seconds, Fetched: 3 row(s)
hive> SELECT * FROM air_quality WHERE Date1 LIKE '10%' limit 3;
OK
10-03-04        18:00:00        2.6     1360    150     11.9    1046    166     1056    113     1692    1268    13.6    48.9    0.7578
10-03-04        19:00:00        2.0     1292    112     9.4     955     103     1174    92      1559    972     13.3    47.7    0.7255
10-03-04        20:00:00        2.2     1402    88      9.0     939     131     1140    114     1555    1074    11.9    54.0    0.7502
Time taken: 0.218 seconds, Fetched: 3 row(s)
hive> SELECT * FROM air_quality WHERE NO2_GT IN (80, 90, 100) limit 3
    > ;
OK
15-03-04        6:00:00 1.4     1157    51      6.4     830     138     1030    80      1584    1083    11.4    70.5    0.9475
17-03-04        1:00:00 1.2     1072    47      5.4     784     95      1066    90      1442    1114    15.5    51.9    0.9059
21-03-04        1:00:00 2.1     1300    -200    7.4     872     133     941     90      1577    1054    15.1    60.9    1.0398
Time taken: 0.184 seconds, Fetched: 3 row(s)

hive> SELECT count(*) FROM air_quality WHERE AH IS NULL;

Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 12.69 sec   HDFS Read: 778470 HDFS Write: 103 SUCCESS
Total MapReduce CPU Time Spent: 12 seconds 690 msec
OK
114
Time taken: 24.379 seconds, Fetched: 1 row(s)




 
8. show and example of regex operation

hive> SELECT *
    > FROM air_quality
    > WHERE Date1 RLIKE '^..-10-..$' limit 3;
OK
01-10-04        0:00:00 1.9     1176    -200    10.4    993     206     659     91      1572    1178    20.3    66.4    1.5591
01-10-04        1:00:00 1.6     1137    -200    9.7     967     140     687     74      1557    1091    19.4    69.3    1.5418
01-10-04        2:00:00 1.3     1035    -200    6.9     851     132     764     69      1465    1002    19.0    70.8    1.5417
Time taken: 0.134 seconds, Fetched: 3 row(s)






9. alter table operation

hive> ALTER TABLE air_quality RENAME TO air_quality_new;
OK
Time taken: 0.162 seconds
hive> show tables;
OK
air_quality_new
 




10 . drop table operation

hive> drop table air_quality_new;
OK
Time taken: 0.193 seconds




12 . order by operation . 

hive> SELECT * FROM air_quality_new WHERE AH IS not NULL order by AH desc limit 5;

Total MapReduce CPU Time Spent: 19 seconds 280 msec
OK
12-09-04        16:00:00        -200.0  1254    -200    6.6     840     -200    787     -200    1602    846     26.1    66.8    2.231
08-08-04        12:00:00        1.4     1032    -200    4.6     744     87      813     79      1695    778     25.4    68.0    2.1806
12-09-04        13:00:00        -200.0  1243    -200    7.6     883     -200    745     -200    1660    1036    24.8    70.4    2.1766
12-08-04        8:00:00 2.2     1205    -200    14.1    1118    122     585     65      2019    1189    27.5    60.1    2.1719
13-08-04        0:00:00 1.4     1033    -200    7.0     856     60      774     62      1736    769     27.4    59.4    2.1395
Time taken: 24.59 seconds, Fetched: 5 row(s)






13 . where clause operations you have to perform 

ive> SELECT * FROM air_quality_new WHERE AH IS not NULL order by AH desc limit 5;

Total MapReduce CPU Time Spent: 19 seconds 280 msec
OK
12-09-04        16:00:00        -200.0  1254    -200    6.6     840     -200    787     -200    1602    846     26.1    66.8    2.231
08-08-04        12:00:00        1.4     1032    -200    4.6     744     87      813     79      1695    778     25.4    68.0    2.1806
12-09-04        13:00:00        -200.0  1243    -200    7.6     883     -200    745     -200    1660    1036    24.8    70.4    2.1766
12-08-04        8:00:00 2.2     1205    -200    14.1    1118    122     585     65      2019    1189    27.5    60.1    2.1719
13-08-04        0:00:00 1.4     1033    -200    7.0     856     60      774     62      1736    769     27.4    59.4    2.1395
Time taken: 24.59 seconds, Fetched: 5 row(s)
. 




14 . sorting operation you have to perform .

hive> SELECT * FROM air_quality_new WHERE AH IS not NULL order by AH desc limit 5;

Total MapReduce CPU Time Spent: 19 seconds 280 msec
OK
12-09-04        16:00:00        -200.0  1254    -200    6.6     840     -200    787     -200    1602    846     26.1    66.8    2.231
08-08-04        12:00:00        1.4     1032    -200    4.6     744     87      813     79      1695    778     25.4    68.0    2.1806
12-09-04        13:00:00        -200.0  1243    -200    7.6     883     -200    745     -200    1660    1036    24.8    70.4    2.1766
12-08-04        8:00:00 2.2     1205    -200    14.1    1118    122     585     65      2019    1189    27.5    60.1    2.1719
13-08-04        0:00:00 1.4     1033    -200    7.0     856     60      774     62      1736    769     27.4    59.4    2.1395
Time taken: 24.59 seconds, Fetched: 5 row(s)
 




15 . distinct operation you have to perform . 

hive> select count(distinct(date1)) from air_quality_new;
Total MapReduce CPU Time Spent: 4 seconds 660 msec
OK
392
Time taken: 23.528 seconds, Fetched: 1 row(s)






16 . like an operation you have to perform . 

hive> SELECT * FROM air_quality WHERE Date1 LIKE '10%' limit 3;
OK
10-03-04        18:00:00        2.6     1360    150     11.9    1046    166     1056    113     1692    1268    13.6    48.9    0.7578
10-03-04        19:00:00        2.0     1292    112     9.4     955     103     1174    92      1559    972     13.3    47.7    0.7255
10-03-04        20:00:00        2.2     1402    88      9.0     939     131     1140    114     1555    1074    11.9    54.0    0.7502
Time taken: 0.218 seconds, Fetched: 3 row(s)





17 . union operation you have to perform . 

SELECT *
FROM air_quality_new
UNION
SELECT *
FROM sample_table;





18 . table view operation you have to perform . 

hive> create view highNO2 as
    > select date1, PT08_S4_NO2,C6H6_GT, CO_GT,time1, NO2_GT from air_quality
    > where NO2_GT>250;
OK

Total MapReduce CPU Time Spent: 16 seconds 100 msec
OK
62
Time taken: 25.681 seconds, Fetched: 1 row(s)

hive> select * from highNO2 limit 5
    > ;
OK
17-11-04        1946    35.9    7.8     17:00:00        272
17-11-04        1796    30.1    6.6     19:00:00        288
17-11-04        1557    22.0    4.3     20:00:00        254
17-11-04        1531    19.0    4.9     21:00:00        267
18-11-04        1560    22.9    5.6     10:00:00        282
Time taken: 0.172 seconds, Fetched: 5 row(s)





hive operation with python

Q: Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations

Pyhton Code to connect with hive


from pyhive import hive

# Connect to Hive
conn = hive.Connection(host='localhost', port=10000, auth='NOSASL', database='your_database')

# Create a cursor
cursor = conn.cursor()

# Execute Hive queries

# Extract data from a table
cursor.execute('SELECT * FROM your_table')

# Fetch rows into a list of tuples
rows = cursor.fetchall()

# Print the fetched rows
for row in rows:
    print(row)

# Create a sub-table for data processing
cursor.execute('CREATE TABLE sub_table AS SELECT column1, column2 FROM your_table WHERE condition')

# Perform join operation
cursor.execute('SELECT * FROM your_table JOIN sub_table ON your_table.id = sub_table.id')

# Fetch rows into a list of tuples from the join result
join_rows = cursor.fetchall()

# Perform filter operation
cursor.execute('SELECT * FROM your_table WHERE condition')

# Fetch rows into a list of tuples from the filter result
filter_rows = cursor.fetchall()

# Drop temporary tables
cursor.execute('DROP TABLE sub_table')

# Close the cursor and the connection
cursor.close()
conn.close()


